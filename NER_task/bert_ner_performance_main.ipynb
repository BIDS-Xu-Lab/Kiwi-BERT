{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5d89463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset,Dataset\n",
    "from glob import glob\n",
    "import os,torch,random\n",
    "from shutil import copyfile\n",
    "cache_dir = '/Users/yr255/Downloads/kiwi_development/data/'\n",
    "model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27f77b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dataset = load_dataset(\"wnut_17\",cache_dir=cache_dir)\n",
    "label_list = ['O','B-problem','I-problem','B-treatment','I-treatment','B-test','I-test','B-drug','I-drug']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9fa831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(files):\n",
    "    data_dict = {'id':[],'tokens':[],'ner_tags':[]}\n",
    "    i=0\n",
    "    for file in files:\n",
    "        with open(file,'r') as f:\n",
    "            lines = f.read().splitlines()\n",
    "            tokens = []\n",
    "            tags = []\n",
    "            for line in lines:\n",
    "                if line != '':\n",
    "                    token,tag = line.split('\\t')\n",
    "                    tokens.append(token)\n",
    "                    tags.append(label_list.index(tag.replace('B-temporal_expression','O').replace('I-temporal_expression','O').replace('B-social_circumstance','O').replace('I-social_circumstance','O')))\n",
    "                else:\n",
    "                    data_dict['id'].append(str(i))\n",
    "                    data_dict['tokens'].append(tokens)\n",
    "                    data_dict['ner_tags'].append(tags)\n",
    "                    tokens = []\n",
    "                    tags = []\n",
    "                    i+=1\n",
    "            data_dict['id'].append(str(i))\n",
    "            data_dict['tokens'].append(tokens)\n",
    "            data_dict['ner_tags'].append(tags)\n",
    "            tokens = []\n",
    "            tags = []\n",
    "            i+=1\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87b42233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7bbde34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa716e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {}\n",
    "for i,label in enumerate(label_list):\n",
    "    id2label.update({i:label})\n",
    "label2id = {}\n",
    "for i,label in enumerate(label_list):\n",
    "    label2id.update({label:i})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12329e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7caeb4df55d34f6681e24e76b1076618",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c08581b49b248f4a7d09abb88520c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/m3/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/envs/m3/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/gw/rzv539755mx1ncdf30k8lvbszl3zcm/T/ipykernel_35650/3471916079.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "train_files = glob('/Users/yr255/Downloads/kiwi_development/data/NER_main_BERT_train.bio')\n",
    "valid_files = glob('/Users/yr255/Downloads/kiwi_development/data/NER_main_BERT_dev.bio')\n",
    "train = Dataset.from_dict(data_loader(train_files))\n",
    "valid = Dataset.from_dict(data_loader(valid_files))\n",
    "\n",
    "template_dataset['train'] = train\n",
    "template_dataset['validation'] = valid\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "tokenized_dataset = template_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(label_list), id2label=id2label, label2id=label2id,cache_dir=cache_dir\n",
    ")\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./models/main_NER/\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    do_train = True,\n",
    "    do_eval = True,\n",
    "    do_predict = True,\n",
    "    metric_for_best_model = 'f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")\n",
    "\n",
    "#trainer.train()\n",
    "#trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae433d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "700fadcd",
   "metadata": {},
   "source": [
    "### individual test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "127bd9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04c6bb23b01497aab495760d0d08146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3e3a30a33f4a98a39564dc8be7edae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/m3/lib/python3.10/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/var/folders/gw/rzv539755mx1ncdf30k8lvbszl3zcm/T/ipykernel_35650/1813036792.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,cache_dir=cache_dir)\n",
    "tokenized_dataset = template_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "model_dir = \"/Users/yr255/Downloads/kiwi_development/BERT/models/main_NER/\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_dir, num_labels=len(label_list), id2label=id2label, label2id=label2id,cache_dir=cache_dir\n",
    ")\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=20,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    do_train = False,\n",
    "    do_eval = False,\n",
    "    do_predict = True,\n",
    "    metric_for_best_model = 'f1'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1186d7ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d7837243e3d49c5ad5c64f258cfebec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4cf4f33a128451fad4bcaf7be30a341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14502 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a98d634c57f44efebc15098e9dfa8139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  Â·Â·Â·Â·Â·Â·Â·Â·\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/yr255/.netrc\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/yr255/Downloads/kiwi_development/BERT/wandb/run-20241119_122438-3mfe19mf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kevin_ren_yang-university-of-south-carolina123/huggingface/runs/3mfe19mf' target=\"_blank\">/Users/yr255/Downloads/kiwi_development/BERT/models/main_NER/</a></strong> to <a href='https://wandb.ai/kevin_ren_yang-university-of-south-carolina123/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kevin_ren_yang-university-of-south-carolina123/huggingface' target=\"_blank\">https://wandb.ai/kevin_ren_yang-university-of-south-carolina123/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kevin_ren_yang-university-of-south-carolina123/huggingface/runs/3mfe19mf' target=\"_blank\">https://wandb.ai/kevin_ren_yang-university-of-south-carolina123/huggingface/runs/3mfe19mf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ./output/: File exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluate: ./output/BERT_main_output_test_prediction_i2b2.bio /Users/yr255/Downloads/kiwi_development/merged_gold_pred.eval_score.txt tab ner False \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "P(exact)\tR(exact)\tF1(exact)\tP(relax)\tR(relax)\tF1(relax)\tright\tright_predict\tright_gold\tpredict\tgold\tSemantic\n",
      "0.853\t0.823\t0.838\t0.958\t0.917\t0.937\t623\t699\t694\t730\t757\tdrug\n",
      "0.794\t0.810\t0.802\t0.904\t0.925\t0.915\t1279\t1456\t1461\t1610\t1579\tproblem\n",
      "0.814\t0.824\t0.819\t0.895\t0.911\t0.903\t1162\t1277\t1286\t1427\t1411\ttest\n",
      "0.636\t0.639\t0.637\t0.756\t0.764\t0.760\t281\t334\t336\t442\t440\ttreatment\n",
      "0.795\t0.799\t0.797\t0.895\t0.902\t0.898\t3345\t3766\t3777\t4209\t4187\toverall\n",
      "evaluate done.\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['i2b2']:\n",
    "\n",
    "    test_files = glob(f'/Users/yr255/Downloads/kiwi_development/data/NER_main_BERT_test_{dataset}.bio')\n",
    "    test = Dataset.from_dict(data_loader(test_files))\n",
    "    template_dataset['test'] = test\n",
    "    tokenized_dataset = template_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "    predictions, labels, metrics = trainer.predict(tokenized_dataset['test'], metric_key_prefix=\"predict\")\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    !mkdir ./output/\n",
    "    with open(f'./output/BERT_main_output_test_prediction_{dataset}.bio','w') as f:\n",
    "        for sentence, predictions,golds in zip(template_dataset['test']['tokens'], true_predictions, template_dataset['test']['ner_tags']):\n",
    "            for token,tag,gold in zip(sentence,predictions,golds):\n",
    "                f.write(f'{token}\\t{label_list[gold]}\\t{tag}\\n')\n",
    "            f.write('\\n')\n",
    "    !python ../evaluate_jianfu_original.py -lf ./output/BERT_main_output_test_prediction_{dataset}.bio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeddbdec-63aa-4da3-878e-1e7ec797a82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
